{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149bb7b5",
   "metadata": {},
   "source": [
    "# Universal Adversarial Audio Attack on Whisper\n",
    "\n",
    "**Proof of Concept**: Creating a universal noise pattern that, when added to any audio, causes Whisper to mistranscribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import whisper\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a939da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LIBRISPEECH_PATH = Path(\"LibriSpeech/test-clean\")\n",
    "SAMPLE_RATE = 16000  # Whisper's expected sample rate\n",
    "MAX_SAMPLES = 25     # Number of audio samples to test (reduced for faster testing)\n",
    "MODEL = \"base\"  # Whisper model size, can be \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "\n",
    "# General Attack Parameters\n",
    "ITERATIONS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "EPSILON = 0.02\n",
    "\n",
    "# Fast Feature Fool (FFF) specific parameters\n",
    "ATTACK_LAYERS = [\n",
    "    'encoder.conv1',\n",
    "    'encoder.conv2',\n",
    "    'encoder.blocks.2.attn',\n",
    "    'encoder.blocks.4.attn'\n",
    "]\n",
    "\n",
    "# PSP-UAP specific parameters\n",
    "PSP_LAMBDA_PERCEPTUAL = 1.0  # Weight for the perceptual loss component\n",
    "PSP_LAMBDA_FOOLING = 1.0     # Weight for the fooling loss component\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(MODEL)\n",
    "print(f\"Loaded Whisper {MODEL} model\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152abbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_librispeech_samples(data_path, max_samples=20):\n",
    "    \"\"\"Load audio files and transcriptions from LibriSpeech dataset\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    if not data_path.exists():\n",
    "        return samples\n",
    "\n",
    "    # Iterate through speaker directories\n",
    "    for speaker_dir in data_path.iterdir():\n",
    "        if not speaker_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        # Iterate through chapter directories\n",
    "        for chapter_dir in speaker_dir.iterdir():\n",
    "            if not chapter_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Load transcription file\n",
    "            speaker_id = speaker_dir.name\n",
    "            chapter_id = chapter_dir.name\n",
    "            trans_file = chapter_dir / f\"{speaker_id}-{chapter_id}.trans.txt\"\n",
    "            if not trans_file.exists():\n",
    "                continue\n",
    "\n",
    "            # Parse transcriptions\n",
    "            transcriptions = {}\n",
    "            with open(trans_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(' ', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        transcriptions[parts[0]] = parts[1]\n",
    "\n",
    "            # Load audio files\n",
    "            for audio_file in chapter_dir.glob(\"*.flac\"):\n",
    "                if len(samples) >= max_samples:\n",
    "                    return samples\n",
    "\n",
    "                file_id = audio_file.stem\n",
    "                if file_id in transcriptions:\n",
    "                    try:\n",
    "                        audio, _ = librosa.load(audio_file, sr=SAMPLE_RATE)\n",
    "                        samples.append({\n",
    "                            'id': file_id,\n",
    "                            'audio': audio,\n",
    "                            'transcript': transcriptions[file_id],\n",
    "                            'path': str(audio_file)\n",
    "                        })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Load samples\n",
    "samples = load_librispeech_samples(LIBRISPEECH_PATH, MAX_SAMPLES)\n",
    "print(f\"Loaded {len(samples)} audio samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ddaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastFeatureFoolAttack:\n",
    "    \"\"\"\n",
    "    Fast Feature Fool (FFF) attack adapted for Whisper.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, attack_layers, epsilon=0.02, learning_rate=0.01, iterations=100,\n",
    "                 data_assisted=False, batch_size=4, log_eps=1e-6):\n",
    "        self.model = model\n",
    "        self.attack_layers = attack_layers\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.data_assisted = data_assisted\n",
    "        self.batch_size = batch_size\n",
    "        self.log_eps = log_eps\n",
    "        self.hooks = []\n",
    "        self.features = {}\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def _get_feature_hook(self, name):\n",
    "        def hook(model, input, output):\n",
    "            # Store post-module activation tensor\n",
    "            self.features[name] = output[0] if isinstance(output, tuple) else output\n",
    "        return hook\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.attack_layers:\n",
    "                self.hooks.append(module.register_forward_hook(self._get_feature_hook(name)))\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_needed_length(self, audio_samples):\n",
    "        if audio_samples:\n",
    "            return max(len(a) for a in audio_samples)\n",
    "        # Fallback to Whisper context length if no samples\n",
    "        return whisper.audio.N_SAMPLES\n",
    "\n",
    "    def _forward_with_signal(self, signal):\n",
    "        \"\"\"Run encoder forward pass given a 1-D waveform tensor (already on device).\"\"\"\n",
    "        n_samples_ctx = whisper.audio.N_SAMPLES\n",
    "        if signal.shape[0] > n_samples_ctx:\n",
    "            sig = signal[:n_samples_ctx]\n",
    "        else:\n",
    "            sig = torch.nn.functional.pad(signal, (0, n_samples_ctx - signal.shape[0]))\n",
    "        mel = whisper.log_mel_spectrogram(sig)\n",
    "        _ = self.model.encoder(mel.unsqueeze(0).to(self.device))\n",
    "\n",
    "    def generate(self, audio_samples=None, sample_rate=16000):\n",
    "        print(\"Generating universal perturbation with Fast Feature Fool (data_free={} )...\".format(\n",
    "            not self.data_assisted))\n",
    "        if audio_samples is None:\n",
    "            audio_samples = []\n",
    "        self._register_hooks()\n",
    "\n",
    "        target_len = self._estimate_needed_length(audio_samples)\n",
    "        # Initialize perturbation uniformly within [-epsilon, epsilon]\n",
    "        perturbation = torch.empty(target_len, device=self.device).uniform_(-self.epsilon, self.epsilon)\n",
    "        perturbation.requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([perturbation], lr=self.learning_rate)\n",
    "\n",
    "        pbar = tqdm(range(self.iterations), desc=\"FFF Iterations\")\n",
    "        for _ in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            self.features.clear()\n",
    "\n",
    "            if self.data_assisted and audio_samples:\n",
    "                # Choose a mini-batch and accumulate activations after applying Î´\n",
    "                batch = random.sample(audio_samples, min(len(audio_samples), self.batch_size))\n",
    "                for audio in batch:\n",
    "                    audio_tensor = torch.tensor(audio, dtype=torch.float32, device=self.device)\n",
    "                    if len(perturbation) >= len(audio_tensor):\n",
    "                        pert = perturbation[:len(audio_tensor)]\n",
    "                    else:\n",
    "                        # Tile perturbation\n",
    "                        repeats = (len(audio_tensor) + len(perturbation) - 1) // len(perturbation)\n",
    "                        pert = perturbation.repeat(repeats)[:len(audio_tensor)]\n",
    "                    attacked = torch.clamp(audio_tensor + pert, -1.0, 1.0)\n",
    "                    self._forward_with_signal(attacked)\n",
    "            else:\n",
    "                # Pure data-free mode: feed perturbation as a standalone waveform (optionally add silence)\n",
    "                self._forward_with_signal(torch.clamp(perturbation, -self.epsilon, self.epsilon))\n",
    "\n",
    "            # Compute paper-aligned objective: maximize product of mean activations\n",
    "            # => minimize negative sum log(mean + eps)\n",
    "            if not self.features:\n",
    "                # Safety: avoid zero division\n",
    "                continue\n",
    "            log_means = []\n",
    "            for name, feat in self.features.items():\n",
    "                # ReLU may have been applied inside layer; ensure non-negativity for log stability\n",
    "                mean_act = torch.mean(torch.relu(feat).float())\n",
    "                log_means.append(torch.log(mean_act + self.log_eps))\n",
    "            obj = -torch.stack(log_means).sum()  # negative so we minimize\n",
    "            obj.backward()\n",
    "            optimizer.step()\n",
    "            # Enforce L_infinity bound\n",
    "            with torch.no_grad():\n",
    "                perturbation.clamp_(-self.epsilon, self.epsilon)\n",
    "            pbar.set_postfix({\"loss\": float(obj.detach().cpu())})\n",
    "\n",
    "        self._remove_hooks()\n",
    "        print(\"Finished generating FFF perturbation.\")\n",
    "        return perturbation.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class GDUAPAttack:\n",
    "    \"\"\"\n",
    "    GD-UAP (Generalizable Data-free UAP) adapted to Whisper (data-free objective).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, attack_layers=None, epsilon=0.02, learning_rate=0.01, iterations=100,\n",
    "                 prior='gaussian', samples_per_iter=4, log_eps=1e-6, seed=0):\n",
    "        self.model = model\n",
    "        self.attack_layers = attack_layers or []\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.prior = prior\n",
    "        self.samples_per_iter = samples_per_iter\n",
    "        self.log_eps = log_eps\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.hooks = []\n",
    "        self.features = {}\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def _get_feature_hook(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.features.setdefault(name, [])\n",
    "            self.features[name].append(output[0] if isinstance(output, tuple) else output)\n",
    "        return hook\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        if not self.attack_layers:\n",
    "            # default: use same layers as FFF if not provided\n",
    "            for name, _ in self.model.named_modules():\n",
    "                if any(k in name for k in ['encoder.conv', 'encoder.blocks']):\n",
    "                    self.attack_layers.append(name)\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.attack_layers:\n",
    "                self.hooks.append(module.register_forward_hook(self._get_feature_hook(name)))\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    def _sample_prior(self, length):\n",
    "        if self.prior == 'gaussian':\n",
    "            return torch.randn(length, device=self.device) * 0.05  # small variance\n",
    "        if self.prior == 'uniform':\n",
    "            return torch.empty(length, device=self.device).uniform_(-0.05, 0.05)\n",
    "        if self.prior == 'pink':\n",
    "            # Fallback simple pink-like noise using FFT if available\n",
    "            try:\n",
    "                freqs = torch.fft.rfft(torch.randn(length, device=self.device))\n",
    "                mag = torch.linspace(1, freqs.shape[0], freqs.shape[0], device=self.device)\n",
    "                freqs = freqs / mag.clamp_min(1.0)\n",
    "                signal = torch.fft.irfft(freqs, n=length)\n",
    "                return signal / signal.abs().max().clamp_min(1e-6) * 0.05\n",
    "            except Exception:\n",
    "                return torch.randn(length, device=self.device) * 0.05\n",
    "        return torch.zeros(length, device=self.device)\n",
    "\n",
    "    def _forward(self, waveform):\n",
    "        n_ctx = whisper.audio.N_SAMPLES\n",
    "        if waveform.numel() > n_ctx:\n",
    "            w = waveform[:n_ctx]\n",
    "        else:\n",
    "            w = torch.nn.functional.pad(waveform, (0, n_ctx - waveform.numel()))\n",
    "        mel = whisper.log_mel_spectrogram(w)\n",
    "        _ = self.model.encoder(mel.unsqueeze(0))\n",
    "\n",
    "    def generate(self, audio_samples=None, sample_rate=16000):\n",
    "        print(\"Generating universal perturbation with GD-UAP (data-free)...\")\n",
    "        target_len = whisper.audio.N_SAMPLES if not audio_samples else max(len(s['audio']) for s in audio_samples)\n",
    "        perturbation = torch.empty(target_len, device=self.device).uniform_(-self.epsilon, self.epsilon)\n",
    "        perturbation.requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([perturbation], lr=self.learning_rate)\n",
    "        self._register_hooks()\n",
    "\n",
    "        pbar = tqdm(range(self.iterations), desc=\"GD-UAP Iterations\")\n",
    "        for _ in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            self.features.clear()\n",
    "            # Draw prior samples\n",
    "            for _k in range(self.samples_per_iter):\n",
    "                z = self._sample_prior(target_len)\n",
    "                x = (z + perturbation).clamp(-1.0, 1.0)\n",
    "                self._forward(x)\n",
    "            # Compute loss\n",
    "            losses = []\n",
    "            for name, acts_list in self.features.items():\n",
    "                acts = torch.stack([torch.relu(a).float().mean() for a in acts_list])  # scalar per sample\n",
    "                mean_act = acts.mean()\n",
    "                losses.append(torch.log(mean_act + self.log_eps))\n",
    "            if not losses:\n",
    "                continue\n",
    "            obj = -torch.stack(losses).sum()\n",
    "            obj.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                perturbation.clamp_(-self.epsilon, self.epsilon)\n",
    "            pbar.set_postfix({\"loss\": float(obj.detach().cpu())})\n",
    "\n",
    "        self._remove_hooks()\n",
    "        print(\"Finished generating GD-UAP perturbation.\")\n",
    "        return perturbation.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class PSPUAPAttack:\n",
    "    \"\"\"\n",
    "    PSP-UAP (Pseudo-Semantic Prior UAP) adapted to audio / Whisper.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, attack_layers=None, epsilon=0.02, learning_rate=0.01, iterations=100,\n",
    "                 semantic_samples=4, crop_min_ratio=0.4, crop_max_ratio=0.9,\n",
    "                 log_eps=1e-6, kl_eps=1e-8, seed=0):\n",
    "        self.model = model\n",
    "        self.attack_layers = attack_layers or []\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.semantic_samples = semantic_samples\n",
    "        self.crop_min_ratio = crop_min_ratio\n",
    "        self.crop_max_ratio = crop_max_ratio\n",
    "        self.log_eps = log_eps\n",
    "        self.kl_eps = kl_eps\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.hooks = []\n",
    "        self.features = {}\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.tokenizer = whisper.tokenizer.get_tokenizer(model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "        if hasattr(self.tokenizer, 'sot_sequence'):\n",
    "            try:\n",
    "                self.sot_tokens = list(self.tokenizer.sot_sequence())\n",
    "            except TypeError:\n",
    "                self.sot_tokens = list(self.tokenizer.sot_sequence)\n",
    "        elif hasattr(self.tokenizer, 'sot'):\n",
    "            self.sot_tokens = [self.tokenizer.sot]\n",
    "        else:\n",
    "            self.sot_tokens = [getattr(self.tokenizer, 'startoftranscript', 50258)]\n",
    "\n",
    "    def _get_feature_hook(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.features.setdefault(name, [])\n",
    "            self.features[name].append(output[0] if isinstance(output, tuple) else output)\n",
    "        return hook\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        if not self.attack_layers:\n",
    "            for name, _ in self.model.named_modules():\n",
    "                if any(k in name for k in ['encoder.conv', 'encoder.blocks']):\n",
    "                    self.attack_layers.append(name)\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.attack_layers:\n",
    "                self.hooks.append(module.register_forward_hook(self._get_feature_hook(name)))\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    def _time_scale(self, x, scale):\n",
    "        import torch.nn.functional as F\n",
    "        target_len = max(1, int(round(x.numel() * scale)))\n",
    "        x2 = x.view(1, 1, -1)\n",
    "        scaled = F.interpolate(x2, size=target_len, mode='linear', align_corners=False).view(-1)\n",
    "        return scaled\n",
    "\n",
    "    def _time_shift(self, x, shift):\n",
    "        shift = shift % x.numel()\n",
    "        if shift == 0:\n",
    "            return x\n",
    "        return torch.cat([x[-shift:], x[:-shift]])\n",
    "\n",
    "    def _block_shuffle(self, x, m=8):\n",
    "        L = x.numel()\n",
    "        blk_len = L // m\n",
    "        if blk_len == 0:\n",
    "            return x\n",
    "        trimmed = x[:blk_len * m].view(m, blk_len)\n",
    "        perm = torch.randperm(m, device=x.device)\n",
    "        shuffled = trimmed[perm].reshape(-1)\n",
    "        if shuffled.numel() < L:\n",
    "            shuffled = torch.cat([shuffled, x[shuffled.numel():]])\n",
    "        return shuffled\n",
    "\n",
    "    def _apply_random_transform(self, x):\n",
    "        choice = random.choice(['scale', 'shift', 'shuffle'])\n",
    "        if choice == 'scale':\n",
    "            scale = random.uniform(0.8, 1.2)\n",
    "            out = self._time_scale(x, scale)\n",
    "            if out.numel() > x.numel():\n",
    "                out = out[:x.numel()]\n",
    "            elif out.numel() < x.numel():\n",
    "                out = torch.nn.functional.pad(out, (0, x.numel() - out.numel()))\n",
    "            return out\n",
    "        if choice == 'shift':\n",
    "            shift = random.randint(0, x.numel()-1)\n",
    "            return self._time_shift(x, shift)\n",
    "        m = 2**int(np.floor(np.log2( max(2, x.numel() // 1000) )))\n",
    "        m = max(2, min(32, m))\n",
    "        return self._block_shuffle(x, m)\n",
    "\n",
    "    def _forward_encoder(self, waveform):\n",
    "        n_ctx = whisper.audio.N_SAMPLES\n",
    "        if waveform.numel() > n_ctx:\n",
    "            w = waveform[:n_ctx]\n",
    "        else:\n",
    "            w = torch.nn.functional.pad(waveform, (0, n_ctx - waveform.numel()))\n",
    "        mel = whisper.log_mel_spectrogram(w)\n",
    "        enc = self.model.encoder(mel.unsqueeze(0))  # (1, T, D)\n",
    "        return enc, mel\n",
    "\n",
    "    def _first_step_probs(self, enc):\n",
    "        if not self.sot_tokens:\n",
    "            raise RuntimeError(\"SOT token list is empty; cannot compute first-step probabilities.\")\n",
    "        sot_ints = [int(t) for t in self.sot_tokens]\n",
    "        tokens_in = torch.tensor([sot_ints], device=self.device, dtype=torch.long)\n",
    "        logits = self.model.decoder(tokens_in, enc)\n",
    "        if isinstance(logits, dict) and 'logits' in logits:\n",
    "            logits = logits['logits']\n",
    "        logits = logits[:, -1, :]\n",
    "        return torch.softmax(logits, dim=-1).squeeze(0)\n",
    "\n",
    "    def generate(self, audio_samples=None, sample_rate=16000):\n",
    "        print(\"Generating universal perturbation with PSP-UAP (pseudo-semantic prior)...\")\n",
    "        target_len = whisper.audio.N_SAMPLES if not audio_samples else max(len(s['audio']) for s in audio_samples)\n",
    "        perturbation = torch.empty(target_len, device=self.device).uniform_(-self.epsilon, self.epsilon)\n",
    "        perturbation.requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([perturbation], lr=self.learning_rate)\n",
    "        self._register_hooks()\n",
    "\n",
    "        pbar = tqdm(range(self.iterations), desc=\"PSP-UAP Iterations\")\n",
    "        for _ in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            self.features.clear()\n",
    "            weights = []\n",
    "            base_noise = torch.randn(target_len, device=self.device) * 0.05\n",
    "            pseudo = (base_noise + perturbation.detach()).clamp(-1.0, 1.0)\n",
    "            for k in range(self.semantic_samples):\n",
    "                ratio = random.uniform(self.crop_min_ratio, self.crop_max_ratio)\n",
    "                seg_len = max(8, int(target_len * ratio))\n",
    "                start = random.randint(0, max(0, target_len - seg_len))\n",
    "                segment = pseudo[start:start+seg_len]\n",
    "                segment_resized = self._time_scale(segment, target_len / segment.numel())\n",
    "                if segment_resized.numel() > target_len:\n",
    "                    segment_resized = segment_resized[:target_len]\n",
    "                elif segment_resized.numel() < target_len:\n",
    "                    segment_resized = torch.nn.functional.pad(segment_resized, (0, target_len - segment_resized.numel()))\n",
    "                transformed = self._apply_random_transform(segment_resized).clamp(-1.0, 1.0)\n",
    "                with torch.no_grad():\n",
    "                    enc_clean, _ = self._forward_encoder(transformed)\n",
    "                    P = self._first_step_probs(enc_clean)\n",
    "                adv_wave = (transformed + perturbation).clamp(-1.0, 1.0)\n",
    "                enc_adv, _ = self._forward_encoder(adv_wave)\n",
    "                Q = self._first_step_probs(enc_adv)\n",
    "                kl = torch.sum(P * (torch.log(P + self.kl_eps) - torch.log(Q + self.kl_eps)))\n",
    "                w = 1.0 / (kl + 1e-6)\n",
    "                weights.append(w)\n",
    "                # Remove clean activation entries so only adversarial activations remain (1 per sample)\n",
    "                for name, acts_list in self.features.items():\n",
    "                    if len(acts_list) >= 2:\n",
    "                        # Each sample adds two entries (clean, adv); drop the clean one (second last after append)\n",
    "                        clean_idx = -2\n",
    "                        if len(acts_list) >= 2:\n",
    "                            acts_list.pop(clean_idx)\n",
    "            layer_terms = []\n",
    "            for name, acts_list in self.features.items():\n",
    "                stacked = [torch.relu(a).float().view(-1) for a in acts_list]\n",
    "                # Sanity: ensure counts match semantic_samples\n",
    "                if len(stacked) != len(weights):\n",
    "                    # Truncate to min length\n",
    "                    m = min(len(stacked), len(weights))\n",
    "                    stacked = stacked[:m]\n",
    "                    weights = weights[:m]\n",
    "                norms = torch.stack([s.norm(2) for s in stacked])\n",
    "                w_vec = torch.stack(weights)\n",
    "                weighted = (w_vec * norms).mean()\n",
    "                layer_terms.append(torch.log(weighted + self.log_eps))\n",
    "            if not layer_terms:\n",
    "                continue\n",
    "            loss = -torch.stack(layer_terms).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                perturbation.clamp_(-self.epsilon, self.epsilon)\n",
    "            pbar.set_postfix({\"loss\": float(loss.detach().cpu())})\n",
    "\n",
    "        self._remove_hooks()\n",
    "        print(\"Finished generating PSP-UAP perturbation.\")\n",
    "        return perturbation.detach().cpu().numpy()\n",
    "\n",
    "print(\"Setting up attacks...\")\n",
    "fff_attack = FastFeatureFoolAttack(model, ATTACK_LAYERS, epsilon=EPSILON, learning_rate=LEARNING_RATE, iterations=ITERATIONS,\n",
    "                                   data_assisted=False)\n",
    "\n",
    "gduap_attack = GDUAPAttack(model, attack_layers=ATTACK_LAYERS, epsilon=EPSILON, learning_rate=LEARNING_RATE, iterations=ITERATIONS)\n",
    "pspuap_attack = PSPUAPAttack(model, attack_layers=ATTACK_LAYERS, epsilon=EPSILON, learning_rate=LEARNING_RATE, iterations=ITERATIONS,\n",
    "                             semantic_samples=3)\n",
    "clean_audio_list = [sample['audio'] for sample in samples]\n",
    "noise_patterns = {\n",
    "    'fff': fff_attack.generate(clean_audio_list, SAMPLE_RATE),\n",
    "    'gduap': gduap_attack.generate(samples, SAMPLE_RATE),\n",
    "    'pspuap': pspuap_attack.generate(samples, SAMPLE_RATE)\n",
    "}\n",
    "print(\"\\nGenerated noise patterns:\")\n",
    "for name, noise in noise_patterns.items():\n",
    "    print(f\"  {name}: amplitude range [{noise.min():.4f}, {noise.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adversarial_noise(audio, noise):\n",
    "    \"\"\"Apply universal noise to audio sample\"\"\"\n",
    "    # Truncate or pad noise to match audio length\n",
    "    if len(noise) > len(audio):\n",
    "        noise_trimmed = noise[:len(audio)]\n",
    "    else:\n",
    "        noise_trimmed = np.pad(noise, (0, len(audio) - len(noise)), mode='wrap')\n",
    "\n",
    "    # Add noise to audio\n",
    "    attacked_audio = audio + noise_trimmed\n",
    "\n",
    "    # Ensure audio stays in valid range [-1, 1]\n",
    "    attacked_audio = np.clip(attacked_audio, -1.0, 1.0)\n",
    "\n",
    "    return attacked_audio\n",
    "\n",
    "# Apply all attack methods to all samples\n",
    "print(\"Applying different adversarial noise patterns to all samples...\")\n",
    "\n",
    "for sample in tqdm(samples, desc=\"Processing samples\"):\n",
    "    # Apply each attack method\n",
    "    for attack_name, noise_pattern in noise_patterns.items():\n",
    "        attacked_audio = apply_adversarial_noise(sample['audio'], noise_pattern)\n",
    "        sample[f'attacked_audio_{attack_name}'] = attacked_audio\n",
    "\n",
    "print(f\"Applied {len(noise_patterns)} different attack methods to all samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39913c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_batch(model, audio_list, batch_name=\"\"):\n",
    "    \"\"\"Transcribe a batch of audio samples using Whisper\"\"\"\n",
    "    transcriptions = []\n",
    "\n",
    "    print(f\"Transcribing {len(audio_list)} {batch_name} samples...\")\n",
    "    for audio in tqdm(audio_list, desc=f\"Transcribing {batch_name}\"):\n",
    "        try:\n",
    "            # Ensure audio is float32 and normalized\n",
    "            audio = np.array(audio, dtype=np.float32)\n",
    "            \n",
    "            # Normalize audio to [-1, 1] range if needed\n",
    "            if np.max(np.abs(audio)) > 1.0:\n",
    "                audio = audio / np.max(np.abs(audio))\n",
    "            \n",
    "            result = model.transcribe(audio, fp16=False)\n",
    "            transcriptions.append(result['text'].strip().upper())\n",
    "        except Exception as e:\n",
    "            print(f\"Transcription error: {e}\")\n",
    "            transcriptions.append(\"\")\n",
    "\n",
    "    return transcriptions\n",
    "\n",
    "# Transcribe clean audio\n",
    "clean_audio_list = [sample['audio'] for sample in samples]\n",
    "clean_transcriptions = transcribe_batch(model, clean_audio_list, \"clean\")\n",
    "\n",
    "# Store clean transcriptions\n",
    "for i, sample in enumerate(samples):\n",
    "    sample['clean_prediction'] = clean_transcriptions[i]\n",
    "\n",
    "# Transcribe attacked audio for each method\n",
    "attack_transcriptions = {}\n",
    "for attack_name in noise_patterns.keys():\n",
    "    attacked_audio_list = [sample[f'attacked_audio_{attack_name}'] for sample in samples]\n",
    "    attack_transcriptions[attack_name] = transcribe_batch(model, attacked_audio_list, f\"attacked ({attack_name})\")\n",
    "    \n",
    "    # Store transcriptions in samples\n",
    "    for i, sample in enumerate(samples):\n",
    "        sample[f'attacked_prediction_{attack_name}'] = attack_transcriptions[attack_name][i]\n",
    "\n",
    "print(\"All transcriptions completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER for all samples and attack methods\n",
    "results = []\n",
    "\n",
    "for sample in samples:\n",
    "    ground_truth = sample['transcript'].upper()\n",
    "    clean_pred = sample['clean_prediction']\n",
    "    clean_wer = wer(ground_truth, clean_pred)\n",
    "    \n",
    "    result_row = {\n",
    "        'id': sample['id'],\n",
    "        'ground_truth': ground_truth,\n",
    "        'clean_prediction': clean_pred,\n",
    "        'clean_wer': clean_wer,\n",
    "    }\n",
    "    \n",
    "    # Add results for each attack method\n",
    "    for attack_name in noise_patterns.keys():\n",
    "        attacked_pred = sample[f'attacked_prediction_{attack_name}']\n",
    "        attacked_wer = wer(ground_truth, attacked_pred)\n",
    "        wer_increase = attacked_wer - clean_wer\n",
    "        \n",
    "        result_row[f'attacked_prediction_{attack_name}'] = attacked_pred\n",
    "        result_row[f'attacked_wer_{attack_name}'] = attacked_wer\n",
    "        result_row[f'wer_increase_{attack_name}'] = wer_increase\n",
    "    \n",
    "    results.append(result_row)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"EVALUATION RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_clean_wer = df_results['clean_wer'].mean()\n",
    "print(f\"Average Clean WER: {avg_clean_wer:.3f} ({avg_clean_wer*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "attack_summary = {}\n",
    "for attack_name in noise_patterns.keys():\n",
    "    avg_attacked_wer = df_results[f'attacked_wer_{attack_name}'].mean()\n",
    "    avg_wer_increase = df_results[f'wer_increase_{attack_name}'].mean()\n",
    "    success_rate = (df_results[f'wer_increase_{attack_name}'] > 0).mean() * 100\n",
    "    \n",
    "    attack_summary[attack_name] = {\n",
    "        'avg_wer': avg_attacked_wer,\n",
    "        'avg_increase': avg_wer_increase,\n",
    "        'success_rate': success_rate\n",
    "    }\n",
    "    \n",
    "    print(f\"{attack_name.upper()} ATTACK:\")\n",
    "    print(f\"  Average WER: {avg_attacked_wer:.3f} ({avg_attacked_wer*100:.1f}%)\")\n",
    "    print(f\"  Average WER Increase: {avg_wer_increase:.3f} ({avg_wer_increase*100:.1f}%)\")\n",
    "    print(f\"  Success Rate: {success_rate:.1f}% (samples with WER increase > 0)\")\n",
    "    print()\n",
    "\n",
    "# Find the most effective attack\n",
    "best_attack = max(attack_summary.items(), key=lambda x: x[1]['avg_increase'])\n",
    "print(f\"MOST EFFECTIVE ATTACK: {best_attack[0].upper()}\")\n",
    "print(f\"  WER Increase: {best_attack[1]['avg_increase']:.3f} ({best_attack[1]['avg_increase']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db279c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WER comparison visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "fig.suptitle('Universal Adversarial Audio Attack - Average WER Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# WER Comparison (Bar Plot)\n",
    "categories = ['Clean'] + [name.upper() for name in noise_patterns.keys()]\n",
    "wer_values = [avg_clean_wer] + [attack_summary[name]['avg_wer'] for name in noise_patterns.keys()]\n",
    "colors = ['lightblue'] + ['lightcoral', 'lightgreen', 'lightsalmon']\n",
    "\n",
    "bars = ax.bar(categories, wer_values, color=colors[:len(categories)], alpha=0.7)\n",
    "ax.set_ylabel('Word Error Rate', fontsize=12)\n",
    "ax.set_title('Average WER Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_ylim(0, max(wer_values) * 1.2 if wer_values else 1)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, val in zip(bars, wer_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfa121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Playback - Listen to Clean vs Attacked Audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def play_audio_comparison(sample_idx=0):\n",
    "    \"\"\"Play clean, attacked, and noise audio for comparison\"\"\"\n",
    "    sample = samples[sample_idx]\n",
    "\n",
    "    # Play clean audio\n",
    "    print(\"\\nCLEAN AUDIO:\")\n",
    "    print(f\"  Ground Truth: {sample['transcript'].upper()}\")\n",
    "    print(f\"  Whisper Prediction: {sample['clean_prediction']}\")\n",
    "    display(Audio(sample['audio'], rate=SAMPLE_RATE))\n",
    "\n",
    "    # Play each attacked version and its corresponding noise\n",
    "    for attack_name, noise_pattern in noise_patterns.items():\n",
    "        attacked_audio = sample[f'attacked_audio_{attack_name}']\n",
    "        print(f\"\\n{attack_name.upper()} ATTACKED AUDIO:\")\n",
    "        print(f\"  Whisper Prediction: {sample[f'attacked_prediction_{attack_name}']}\")\n",
    "        display(Audio(attacked_audio, rate=SAMPLE_RATE))\n",
    "\n",
    "play_audio_comparison(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
