{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149bb7b5",
   "metadata": {},
   "source": [
    "# Universal Adversarial Audio Attack on Whisper\n",
    "\n",
    "**Proof of Concept**: Creating a universal noise pattern that, when added to any audio, causes Whisper to mistranscribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import whisper\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a939da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LIBRISPEECH_PATH = Path(\"LibriSpeech/test-clean\")\n",
    "SAMPLE_RATE = 16000  # Whisper's expected sample rate\n",
    "MAX_SAMPLES = 25     # Number of audio samples to test (reduced for faster testing)\n",
    "MODEL = \"base\"  # Whisper model size, can be \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "\n",
    "# General Attack Parameters\n",
    "ITERATIONS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "EPSILON = 0.02\n",
    "\n",
    "# Fast Feature Fool (FFF) specific parameters\n",
    "ATTACK_LAYERS = [\n",
    "    'encoder.conv1',\n",
    "    'encoder.conv2',\n",
    "    'encoder.blocks.2.attn',\n",
    "    'encoder.blocks.4.attn'\n",
    "]\n",
    "\n",
    "# GD-UAP specific parameters\n",
    "# (Uses general parameters, no specific ones needed for this implementation)\n",
    "\n",
    "# PSP-UAP specific parameters\n",
    "PSP_LAMBDA_PERCEPTUAL = 1.0  # Weight for the perceptual loss component\n",
    "PSP_LAMBDA_FOOLING = 1.0     # Weight for the fooling loss component\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(MODEL)\n",
    "print(f\"Loaded Whisper {MODEL} model\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152abbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_librispeech_samples(data_path, max_samples=20):\n",
    "    \"\"\"Load audio files and transcriptions from LibriSpeech dataset\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    if not data_path.exists():\n",
    "        return samples\n",
    "\n",
    "    # Iterate through speaker directories\n",
    "    for speaker_dir in data_path.iterdir():\n",
    "        if not speaker_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        # Iterate through chapter directories\n",
    "        for chapter_dir in speaker_dir.iterdir():\n",
    "            if not chapter_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Load transcription file\n",
    "            speaker_id = speaker_dir.name\n",
    "            chapter_id = chapter_dir.name\n",
    "            trans_file = chapter_dir / f\"{speaker_id}-{chapter_id}.trans.txt\"\n",
    "            if not trans_file.exists():\n",
    "                continue\n",
    "\n",
    "            # Parse transcriptions\n",
    "            transcriptions = {}\n",
    "            with open(trans_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(' ', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        transcriptions[parts[0]] = parts[1]\n",
    "\n",
    "            # Load audio files\n",
    "            for audio_file in chapter_dir.glob(\"*.flac\"):\n",
    "                if len(samples) >= max_samples:\n",
    "                    return samples\n",
    "\n",
    "                file_id = audio_file.stem\n",
    "                if file_id in transcriptions:\n",
    "                    try:\n",
    "                        audio, _ = librosa.load(audio_file, sr=SAMPLE_RATE)\n",
    "                        samples.append({\n",
    "                            'id': file_id,\n",
    "                            'audio': audio,\n",
    "                            'transcript': transcriptions[file_id],\n",
    "                            'path': str(audio_file)\n",
    "                        })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Load samples\n",
    "samples = load_librispeech_samples(LIBRISPEECH_PATH, MAX_SAMPLES)\n",
    "print(f\"Loaded {len(samples)} audio samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ddaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastFeatureFoolAttack:\n",
    "    \"\"\"\n",
    "    Implementation of the Fast Feature Fool attack for Whisper in PyTorch.\n",
    "    Generates a universal perturbation to maximize activations in specified layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, attack_layers, epsilon=0.02, learning_rate=0.01, iterations=100):\n",
    "        self.model = model\n",
    "        self.attack_layers = attack_layers\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.hooks = []\n",
    "        self.features = {}\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def _get_feature_hook(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.features[name] = output[0] if isinstance(output, tuple) else output\n",
    "        return hook\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.attack_layers:\n",
    "                self.hooks.append(module.register_forward_hook(self._get_feature_hook(name)))\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    def generate(self, audio_samples, sample_rate):\n",
    "        print(\"Generating universal perturbation with Fast Feature Fool...\")\n",
    "        self._register_hooks()\n",
    "        max_length = max(len(audio) for audio in audio_samples)\n",
    "        perturbation = torch.zeros(max_length, requires_grad=True, device=self.device)\n",
    "        optimizer = torch.optim.Adam([perturbation], lr=self.learning_rate)\n",
    "        for _ in tqdm(range(self.iterations), desc=\"FFF Iterations\"):\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            batch_samples = random.sample(audio_samples, min(len(audio_samples), 4))\n",
    "            for audio in batch_samples:\n",
    "                audio_tensor = torch.tensor(audio, dtype=torch.float32, device=self.device)\n",
    "                if len(perturbation) > len(audio_tensor):\n",
    "                    pert = perturbation[:len(audio_tensor)]\n",
    "                else:\n",
    "                    n_repeats = (len(audio_tensor) + len(perturbation) - 1) // len(perturbation)\n",
    "                    pert = perturbation.repeat(n_repeats)[:len(audio_tensor)]\n",
    "                attacked_audio = torch.clamp(audio_tensor + pert, -1.0, 1.0)\n",
    "                n_samples = whisper.audio.N_SAMPLES\n",
    "                if attacked_audio.shape[0] > n_samples:\n",
    "                    padded_audio = attacked_audio[:n_samples]\n",
    "                else:\n",
    "                    padded_audio = torch.nn.functional.pad(attacked_audio, (0, n_samples - attacked_audio.shape[0]))\n",
    "                mel = whisper.log_mel_spectrogram(padded_audio).to(self.device)\n",
    "                self.model.encoder(mel.unsqueeze(0))\n",
    "                loss = 0\n",
    "                for name in self.features:\n",
    "                    loss -= torch.norm(self.features[name])\n",
    "                total_loss += loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            perturbation.data = torch.clamp(perturbation.data, -self.epsilon, self.epsilon)\n",
    "        self._remove_hooks()\n",
    "        print(\"Finished generating perturbation.\")\n",
    "        return perturbation.detach().cpu().numpy()\n",
    "\n",
    "class GDUAPAttack:\n",
    "    def __init__(self, model, epsilon=0.02, learning_rate=0.01, iterations=100):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def _build_target_tokens(self, tokenizer, transcript):\n",
    "        prefix = []\n",
    "        sot_seq = getattr(tokenizer, 'sot_sequence', None)\n",
    "        if sot_seq is not None:\n",
    "            if callable(sot_seq):\n",
    "                try:\n",
    "                    sot_seq_val = sot_seq()\n",
    "                except TypeError:\n",
    "                    sot_seq_val = sot_seq\n",
    "            else:\n",
    "                sot_seq_val = sot_seq\n",
    "            prefix = list(sot_seq_val)\n",
    "        elif hasattr(tokenizer, 'sot'):\n",
    "            prefix = [tokenizer.sot]\n",
    "        encoded = tokenizer.encode(transcript)\n",
    "        if hasattr(tokenizer, 'eot'):\n",
    "            encoded = encoded + [tokenizer.eot]\n",
    "        tokens = prefix + encoded\n",
    "        tokens = tokens[: self.model.dims.n_text_ctx]\n",
    "        return torch.tensor(tokens, device=self.device, dtype=torch.long)\n",
    "\n",
    "    def generate(self, audio_samples, sample_rate):\n",
    "        print(\"Generating universal perturbation with GD-UAP...\")\n",
    "        max_length = max(len(s['audio']) for s in audio_samples)\n",
    "        perturbation = torch.zeros(max_length, requires_grad=True, device=self.device)\n",
    "        optimizer = torch.optim.Adam([perturbation], lr=self.learning_rate)\n",
    "        tokenizer = whisper.tokenizer.get_tokenizer(self.model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "        for _ in tqdm(range(self.iterations), desc=\"GD-UAP Iterations\"):\n",
    "            optimizer.zero_grad()\n",
    "            total_obj = 0.0\n",
    "            batch_samples = random.sample(audio_samples, min(len(audio_samples), 4))\n",
    "            for sample in batch_samples:\n",
    "                audio = sample['audio']\n",
    "                transcript = sample['transcript']\n",
    "                audio_tensor = torch.tensor(audio, dtype=torch.float32, device=self.device)\n",
    "                if len(perturbation) > len(audio_tensor):\n",
    "                    pert = perturbation[:len(audio_tensor)]\n",
    "                else:\n",
    "                    n_repeats = (len(audio_tensor) + len(perturbation) - 1) // len(perturbation)\n",
    "                    pert = perturbation.repeat(n_repeats)[:len(audio_tensor)]\n",
    "                attacked_audio = torch.clamp(audio_tensor + pert, -1.0, 1.0)\n",
    "                n_samples = whisper.audio.N_SAMPLES\n",
    "                if attacked_audio.shape[0] > n_samples:\n",
    "                    padded_audio = attacked_audio[:n_samples]\n",
    "                else:\n",
    "                    padded_audio = torch.nn.functional.pad(attacked_audio, (0, n_samples - attacked_audio.shape[0]))\n",
    "                mel = whisper.log_mel_spectrogram(padded_audio).to(self.device)\n",
    "                full_tokens = self._build_target_tokens(tokenizer, transcript)\n",
    "                if full_tokens.shape[0] < 2:\n",
    "                    continue\n",
    "                tokens_in = full_tokens[:-1].unsqueeze(0)\n",
    "                targets = full_tokens[1:]\n",
    "                logits = self.model(mel.unsqueeze(0), tokens_in)\n",
    "                if isinstance(logits, dict) and 'logits' in logits:\n",
    "                    logits = logits['logits']\n",
    "                if isinstance(logits, (list, tuple)) and not torch.is_tensor(logits):\n",
    "                    logits = logits[0]\n",
    "                logits = logits[0, :targets.shape[0], :]\n",
    "                loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "                total_obj += -loss\n",
    "            if isinstance(total_obj, float):\n",
    "                continue\n",
    "            total_obj.backward()\n",
    "            optimizer.step()\n",
    "            perturbation.data = torch.clamp(perturbation.data, -self.epsilon, self.epsilon)\n",
    "        print(\"Finished generating GD-UAP perturbation.\")\n",
    "        return perturbation.detach().cpu().numpy()\n",
    "\n",
    "class PSPUAPAttack:\n",
    "    def __init__(self, model, epsilon=0.02, learning_rate=0.01, iterations=100, lambda_perceptual=1.0, lambda_fooling=1.0):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.lambda_perceptual = lambda_perceptual\n",
    "        self.lambda_fooling = lambda_fooling\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def _perceptual_loss(self, perturbation):\n",
    "        n_fft = 2048\n",
    "        window = torch.hann_window(n_fft, device=self.device)\n",
    "        spec = torch.stft(perturbation, n_fft=n_fft, window=window, return_complex=True)\n",
    "        power_spec = torch.abs(spec) ** 2  # (freq_bins, frames)\n",
    "        freq_bins = power_spec.shape[0]\n",
    "        # Dynamic weighting from low->high frequency\n",
    "        weights = (torch.linspace(0, 1, freq_bins, device=self.device) ** 2)[:, None]  # (freq_bins,1)\n",
    "        perceptual_loss = (power_spec * weights).mean()\n",
    "        return perceptual_loss\n",
    "\n",
    "    def _build_target_tokens(self, tokenizer, transcript):\n",
    "        prefix = []\n",
    "        sot_seq = getattr(tokenizer, 'sot_sequence', None)\n",
    "        if sot_seq is not None:\n",
    "            if callable(sot_seq):\n",
    "                try:\n",
    "                    sot_seq_val = sot_seq()\n",
    "                except TypeError:\n",
    "                    sot_seq_val = sot_seq\n",
    "            else:\n",
    "                sot_seq_val = sot_seq\n",
    "            prefix = list(sot_seq_val)\n",
    "        elif hasattr(tokenizer, 'sot'):\n",
    "            prefix = [tokenizer.sot]\n",
    "        encoded = tokenizer.encode(transcript)\n",
    "        if hasattr(tokenizer, 'eot'):\n",
    "            encoded = encoded + [tokenizer.eot]\n",
    "        tokens = prefix + encoded\n",
    "        tokens = tokens[: self.model.dims.n_text_ctx]\n",
    "        return torch.tensor(tokens, device=self.device, dtype=torch.long)\n",
    "\n",
    "    def generate(self, audio_samples, sample_rate):\n",
    "        print(\"Generating universal perturbation with PSP-UAP...\")\n",
    "        max_length = max(len(s['audio']) for s in audio_samples)\n",
    "        perturbation = torch.zeros(max_length, requires_grad=True, device=self.device)\n",
    "        optimizer = torch.optim.Adam([perturbation], lr=self.learning_rate)\n",
    "        tokenizer = whisper.tokenizer.get_tokenizer(self.model.is_multilingual, language=\"en\", task=\"transcribe\")\n",
    "        for _ in tqdm(range(self.iterations), desc=\"PSP-UAP Iterations\"):\n",
    "            optimizer.zero_grad()\n",
    "            fooling_obj = 0.0\n",
    "            batch_samples = random.sample(audio_samples, min(len(audio_samples), 4))\n",
    "            for sample in batch_samples:\n",
    "                audio = sample['audio']\n",
    "                transcript = sample['transcript']\n",
    "                audio_tensor = torch.tensor(audio, dtype=torch.float32, device=self.device)\n",
    "                if len(perturbation) > len(audio_tensor):\n",
    "                    pert = perturbation[:len(audio_tensor)]\n",
    "                else:\n",
    "                    n_repeats = (len(audio_tensor) + len(perturbation) - 1) // len(perturbation)\n",
    "                    pert = perturbation.repeat(n_repeats)[:len(audio_tensor)]\n",
    "                attacked_audio = torch.clamp(audio_tensor + pert, -1.0, 1.0)\n",
    "                n_samples = whisper.audio.N_SAMPLES\n",
    "                if attacked_audio.shape[0] > n_samples:\n",
    "                    padded_audio = attacked_audio[:n_samples]\n",
    "                else:\n",
    "                    padded_audio = torch.nn.functional.pad(attacked_audio, (0, n_samples - attacked_audio.shape[0]))\n",
    "                mel = whisper.log_mel_spectrogram(padded_audio).to(self.device)\n",
    "                full_tokens = self._build_target_tokens(tokenizer, transcript)\n",
    "                if full_tokens.shape[0] < 2:\n",
    "                    continue\n",
    "                tokens_in = full_tokens[:-1].unsqueeze(0)\n",
    "                targets = full_tokens[1:]\n",
    "                logits = self.model(mel.unsqueeze(0), tokens_in)\n",
    "                if isinstance(logits, dict) and 'logits' in logits:\n",
    "                    logits = logits['logits']\n",
    "                if isinstance(logits, (list, tuple)) and not torch.is_tensor(logits):\n",
    "                    logits = logits[0]\n",
    "                logits = logits[0, :targets.shape[0], :]\n",
    "                ce_loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "                fooling_obj += -ce_loss\n",
    "            if not isinstance(fooling_obj, torch.Tensor):\n",
    "                continue\n",
    "            perceptual = self._perceptual_loss(perturbation)\n",
    "            total_obj = self.lambda_fooling * fooling_obj + self.lambda_perceptual * (-perceptual)\n",
    "            total_obj.backward()\n",
    "            optimizer.step()\n",
    "            perturbation.data = torch.clamp(perturbation.data, -self.epsilon, self.epsilon)\n",
    "        print(\"Finished generating PSP-UAP perturbation.\")\n",
    "        return perturbation.detach().cpu().numpy()\n",
    "\n",
    "print(\"Setting up attacks...\")\n",
    "fff_attack = FastFeatureFoolAttack(model, ATTACK_LAYERS, epsilon=EPSILON, learning_rate=LEARNING_RATE, iterations=ITERATIONS)\n",
    "gduap_attack = GDUAPAttack(model, epsilon=EPSILON, learning_rate=LEARNING_RATE, iterations=ITERATIONS)\n",
    "pspuap_attack = PSPUAPAttack(model, epsilon=EPSILON, learning_rate=LEARNING_RATE, iterations=ITERATIONS, \n",
    "                             lambda_perceptual=PSP_LAMBDA_PERCEPTUAL, lambda_fooling=PSP_LAMBDA_FOOLING)\n",
    "clean_audio_list = [sample['audio'] for sample in samples]\n",
    "noise_patterns = {\n",
    "    'fff': fff_attack.generate(clean_audio_list, SAMPLE_RATE),\n",
    "    'gduap': gduap_attack.generate(samples, SAMPLE_RATE),\n",
    "    'pspuap': pspuap_attack.generate(samples, SAMPLE_RATE)\n",
    "}\n",
    "print(\"\\nGenerated noise patterns:\")\n",
    "for name, noise in noise_patterns.items():\n",
    "    print(f\"  {name}: amplitude range [{noise.min():.4f}, {noise.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adversarial_noise(audio, noise):\n",
    "    \"\"\"Apply universal noise to audio sample\"\"\"\n",
    "    # Truncate or pad noise to match audio length\n",
    "    if len(noise) > len(audio):\n",
    "        noise_trimmed = noise[:len(audio)]\n",
    "    else:\n",
    "        noise_trimmed = np.pad(noise, (0, len(audio) - len(noise)), mode='wrap')\n",
    "\n",
    "    # Add noise to audio\n",
    "    attacked_audio = audio + noise_trimmed\n",
    "\n",
    "    # Ensure audio stays in valid range [-1, 1]\n",
    "    attacked_audio = np.clip(attacked_audio, -1.0, 1.0)\n",
    "\n",
    "    return attacked_audio\n",
    "\n",
    "# Apply all attack methods to all samples\n",
    "print(\"Applying different adversarial noise patterns to all samples...\")\n",
    "\n",
    "for sample in tqdm(samples, desc=\"Processing samples\"):\n",
    "    # Apply each attack method\n",
    "    for attack_name, noise_pattern in noise_patterns.items():\n",
    "        attacked_audio = apply_adversarial_noise(sample['audio'], noise_pattern)\n",
    "        sample[f'attacked_audio_{attack_name}'] = attacked_audio\n",
    "\n",
    "print(f\"Applied {len(noise_patterns)} different attack methods to all samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39913c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_batch(model, audio_list, batch_name=\"\"):\n",
    "    \"\"\"Transcribe a batch of audio samples using Whisper\"\"\"\n",
    "    transcriptions = []\n",
    "\n",
    "    print(f\"Transcribing {len(audio_list)} {batch_name} samples...\")\n",
    "    for audio in tqdm(audio_list, desc=f\"Transcribing {batch_name}\"):\n",
    "        try:\n",
    "            # Ensure audio is float32 and normalized\n",
    "            audio = np.array(audio, dtype=np.float32)\n",
    "            \n",
    "            # Normalize audio to [-1, 1] range if needed\n",
    "            if np.max(np.abs(audio)) > 1.0:\n",
    "                audio = audio / np.max(np.abs(audio))\n",
    "            \n",
    "            result = model.transcribe(audio, fp16=False)\n",
    "            transcriptions.append(result['text'].strip().upper())\n",
    "        except Exception as e:\n",
    "            print(f\"Transcription error: {e}\")\n",
    "            transcriptions.append(\"\")\n",
    "\n",
    "    return transcriptions\n",
    "\n",
    "# Transcribe clean audio\n",
    "clean_audio_list = [sample['audio'] for sample in samples]\n",
    "clean_transcriptions = transcribe_batch(model, clean_audio_list, \"clean\")\n",
    "\n",
    "# Store clean transcriptions\n",
    "for i, sample in enumerate(samples):\n",
    "    sample['clean_prediction'] = clean_transcriptions[i]\n",
    "\n",
    "# Transcribe attacked audio for each method\n",
    "attack_transcriptions = {}\n",
    "for attack_name in noise_patterns.keys():\n",
    "    attacked_audio_list = [sample[f'attacked_audio_{attack_name}'] for sample in samples]\n",
    "    attack_transcriptions[attack_name] = transcribe_batch(model, attacked_audio_list, f\"attacked ({attack_name})\")\n",
    "    \n",
    "    # Store transcriptions in samples\n",
    "    for i, sample in enumerate(samples):\n",
    "        sample[f'attacked_prediction_{attack_name}'] = attack_transcriptions[attack_name][i]\n",
    "\n",
    "print(\"All transcriptions completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER for all samples and attack methods\n",
    "results = []\n",
    "\n",
    "for sample in samples:\n",
    "    ground_truth = sample['transcript'].upper()\n",
    "    clean_pred = sample['clean_prediction']\n",
    "    clean_wer = wer(ground_truth, clean_pred)\n",
    "    \n",
    "    result_row = {\n",
    "        'id': sample['id'],\n",
    "        'ground_truth': ground_truth,\n",
    "        'clean_prediction': clean_pred,\n",
    "        'clean_wer': clean_wer,\n",
    "    }\n",
    "    \n",
    "    # Add results for each attack method\n",
    "    for attack_name in noise_patterns.keys():\n",
    "        attacked_pred = sample[f'attacked_prediction_{attack_name}']\n",
    "        attacked_wer = wer(ground_truth, attacked_pred)\n",
    "        wer_increase = attacked_wer - clean_wer\n",
    "        \n",
    "        result_row[f'attacked_prediction_{attack_name}'] = attacked_pred\n",
    "        result_row[f'attacked_wer_{attack_name}'] = attacked_wer\n",
    "        result_row[f'wer_increase_{attack_name}'] = wer_increase\n",
    "    \n",
    "    results.append(result_row)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"EVALUATION RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_clean_wer = df_results['clean_wer'].mean()\n",
    "print(f\"Average Clean WER: {avg_clean_wer:.3f} ({avg_clean_wer*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "attack_summary = {}\n",
    "for attack_name in noise_patterns.keys():\n",
    "    avg_attacked_wer = df_results[f'attacked_wer_{attack_name}'].mean()\n",
    "    avg_wer_increase = df_results[f'wer_increase_{attack_name}'].mean()\n",
    "    success_rate = (df_results[f'wer_increase_{attack_name}'] > 0).mean() * 100\n",
    "    \n",
    "    attack_summary[attack_name] = {\n",
    "        'avg_wer': avg_attacked_wer,\n",
    "        'avg_increase': avg_wer_increase,\n",
    "        'success_rate': success_rate\n",
    "    }\n",
    "    \n",
    "    print(f\"{attack_name.upper()} ATTACK:\")\n",
    "    print(f\"  Average WER: {avg_attacked_wer:.3f} ({avg_attacked_wer*100:.1f}%)\")\n",
    "    print(f\"  Average WER Increase: {avg_wer_increase:.3f} ({avg_wer_increase*100:.1f}%)\")\n",
    "    print(f\"  Success Rate: {success_rate:.1f}% (samples with WER increase > 0)\")\n",
    "    print()\n",
    "\n",
    "# Find the most effective attack\n",
    "best_attack = max(attack_summary.items(), key=lambda x: x[1]['avg_increase'])\n",
    "print(f\"MOST EFFECTIVE ATTACK: {best_attack[0].upper()}\")\n",
    "print(f\"  WER Increase: {best_attack[1]['avg_increase']:.3f} ({best_attack[1]['avg_increase']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db279c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WER comparison visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "fig.suptitle('Universal Adversarial Audio Attack - Average WER Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# WER Comparison (Bar Plot)\n",
    "categories = ['Clean'] + [name.upper() for name in noise_patterns.keys()]\n",
    "wer_values = [avg_clean_wer] + [attack_summary[name]['avg_wer'] for name in noise_patterns.keys()]\n",
    "colors = ['lightblue'] + ['lightcoral', 'lightgreen', 'lightsalmon']\n",
    "\n",
    "bars = ax.bar(categories, wer_values, color=colors[:len(categories)], alpha=0.7)\n",
    "ax.set_ylabel('Word Error Rate', fontsize=12)\n",
    "ax.set_title('Average WER Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_ylim(0, max(wer_values) * 1.2 if wer_values else 1)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, val in zip(bars, wer_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfa121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Playback - Listen to Clean vs Attacked Audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def play_audio_comparison(sample_idx=0):\n",
    "    \"\"\"Play clean, attacked, and noise audio for comparison\"\"\"\n",
    "    sample = samples[sample_idx]\n",
    "\n",
    "    # Play clean audio\n",
    "    print(\"\\nCLEAN AUDIO:\")\n",
    "    print(f\"  Ground Truth: {sample['transcript'].upper()}\")\n",
    "    print(f\"  Whisper Prediction: {sample['clean_prediction']}\")\n",
    "    display(Audio(sample['audio'], rate=SAMPLE_RATE))\n",
    "\n",
    "    # Play each attacked version and its corresponding noise\n",
    "    for attack_name, noise_pattern in noise_patterns.items():\n",
    "        attacked_audio = sample[f'attacked_audio_{attack_name}']\n",
    "        print(f\"\\n{attack_name.upper()} ATTACKED AUDIO:\")\n",
    "        print(f\"  Whisper Prediction: {sample[f'attacked_prediction_{attack_name}']}\")\n",
    "        display(Audio(attacked_audio, rate=SAMPLE_RATE))\n",
    "        \n",
    "        print(f\"\\n{attack_name.upper()} ADVERSARIAL NOISE:\")\n",
    "        display(Audio(noise_pattern, rate=SAMPLE_RATE))\n",
    "\n",
    "play_audio_comparison(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
